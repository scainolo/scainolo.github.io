"use strict";(globalThis["webpackChunkquasar_tufte"]=globalThis["webpackChunkquasar_tufte"]||[]).push([[734],{7734:(e,a,n)=>{n.r(a),n.d(a,{default:()=>ie});var i=n(9835);const t=e=>((0,i.dD)("data-v-41c6b666"),e=e(),(0,i.Cn)(),e),o={class:"tufte-body"},r=t((()=>(0,i._)("section",null,[(0,i._)("h1",null,"Publications")],-1))),s=(0,i.Uk)(),l=t((()=>(0,i._)("h2",{id:"/publications/#pubs-in-press",class:"no-count"},"In Press",-1))),c=(0,i.Uk)(),d={class:"fullwidth"},u=(0,i.Uk)(),h=t((()=>(0,i._)("h2",{id:"/publications/#pubs-journals",class:"no-count"},"International Journals",-1))),m=(0,i.Uk)(),p={class:"fullwidth"},g=(0,i.Uk)(),f=t((()=>(0,i._)("h2",{id:"/publications/#pubs-conferences",class:"no-count"},"Peer-Reviewed International Conference Proceedings",-1))),y=(0,i.Uk)(),b={class:"fullwidth"},w=(0,i.Uk)(),v=t((()=>(0,i._)("h2",{id:"/publications/#pubs-national-conferences",class:"no-count"},"Peer-Reviewed National Conference Proceedings",-1))),k=(0,i.Uk)(),S={class:"fullwidth"},C=(0,i.Uk)(),A=t((()=>(0,i._)("h2",{id:"/publications/#pubs-posters",class:"no-count"},"Peer-Reviewed Posters and Short Papers",-1))),N=(0,i.Uk)(),P={class:"fullwidth"},E=(0,i.Uk)(),T=t((()=>(0,i._)("h2",{id:"/publications/#pubs-workshops",class:"no-count"},"Peer-Reviewed Workshop Papers",-1))),D=(0,i.Uk)(),M={class:"fullwidth"},I=(0,i.Uk)(),L=t((()=>(0,i._)("h2",{id:"/publications/#pubs-reports",class:"no-count"},"Technical Reports",-1))),R=(0,i.Uk)(),x={class:"fullwidth"},H=(0,i.Uk)(),W=t((()=>(0,i._)("h2",{id:"/publications/#pubs-preprints",class:"no-count"},"Preprints",-1))),B=(0,i.Uk)(),F={class:"fullwidth"},z=(0,i.Uk)(),G=t((()=>(0,i._)("h2",{id:"/publications/#pubs-theses",class:"no-count"},"Theses",-1))),Z=(0,i.Uk)(),J={class:"fullwidth"};function K(e,a,n,t,K,U){const j=(0,i.up)("BibtexBibliography");return(0,i.wg)(),(0,i.iD)("div",o,[(0,i._)("article",null,[r,s,(0,i._)("section",null,[l,c,(0,i._)("p",d,[(0,i.Wm)(j,{bibtex:e.in_press,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),u,(0,i._)("section",null,[h,m,(0,i._)("p",p,[(0,i.Wm)(j,{bibtex:e.journals,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),g,(0,i._)("section",null,[f,y,(0,i._)("p",b,[(0,i.Wm)(j,{bibtex:e.conferences,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),w,(0,i._)("section",null,[v,k,(0,i._)("p",S,[(0,i.Wm)(j,{bibtex:e.national_conferences,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),C,(0,i._)("section",null,[A,N,(0,i._)("p",P,[(0,i.Wm)(j,{bibtex:e.posters,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),E,(0,i._)("section",null,[T,D,(0,i._)("p",M,[(0,i.Wm)(j,{bibtex:e.workshops,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),I,(0,i._)("section",null,[L,R,(0,i._)("p",x,[(0,i.Wm)(j,{bibtex:e.reports,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),H,(0,i._)("section",null,[W,B,(0,i._)("p",F,[(0,i.Wm)(j,{bibtex:e.preprints,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])]),z,(0,i._)("section",null,[G,Z,(0,i._)("p",J,[(0,i.Wm)(j,{bibtex:e.theses,mode:"all",highlightCallback:e.highlighter},null,8,["bibtex","highlightCallback"])])])])])}var U=n(499),j=n(4935).Z,O=n(3342).Z,q=n(8748).Z,_=n(2411).Z,X=n(7531).Z,Y=n(8350).Z,V=n(2169).Z,Q=n(2725).Z,$=n(9818).Z;const ee=(0,i.aZ)({name:"IndexPage",components:{},setup(){const e=(0,U.iH)(j),a=(0,U.iH)(O),n=(0,U.iH)(q),i=(0,U.iH)(_),t=(0,U.iH)(X),o=(0,U.iH)(Y),r=(0,U.iH)(V),s=(0,U.iH)(Q),l=(0,U.iH)($);function c(e){return e=e.replace(/Caino-Lores, Silvina,/g,"<b>Caino-Lores, Silvina</b>,"),e=e.replace(/Caino-Lores, S.,/g,"<b>Caino-Lores, S.</b>,"),e=e.replace(/Caino, S.,/g,"<b>Caino-Lores, S.</b>,"),e=e.replace(/Caíno-Lores, S.,/g,"<b>Caino-Lores, S.</b>,"),e}return{highlighter:c,in_press:e,journals:a,preprints:n,conferences:i,national_conferences:t,posters:o,workshops:r,reports:s,theses:l}}});var ae=n(1639);const ne=(0,ae.Z)(ee,[["render",K],["__scopeId","data-v-41c6b666"]]),ie=ne},4935:(e,a,n)=>{n.d(a,{Z:()=>i});const i=""},2411:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@inproceedings{caino-loresDataAwareSupportHybrid2017,\n  title = {Data-{{Aware Support}} for {{Hybrid HPC}} and {{Big Data Applications}}},\n  booktitle = {2017 17th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}} ({{CCGRID}})},\n  author = {{Caino-Lores}, Silvina and Isaila, Florin and Carretero, Jes{\\'u}s},\n  year = {2017},\n  month = may,\n  pages = {719--722},\n  doi = {10.1109/CCGRID.2017.55},\n  abstract = {Nowadays there is a raising interest in bridging the gap between Big Data application models and data-intensive HPC. This work explores the effects that Big Data-inspired paradigms could have in current scientific applications through the evaluation of a real-world application from the hydrology domain. This evaluation led to experience that portrayed the key aspects of the HPC and Big Data paradigms that made them successful in their respective worlds. With this information, we established a research roadmap to build a platform suitable for HPC hybrid applications, with a focus on efficient data management and fault-tolerance.},\n  keywords = {big data,Cloud computing,Computational modeling,Data analysis,data management,data-intensive computing,high-performance computing,Hydrology,Random access memory,Scalability,scientific computing,Sparks},\n  file = {/home/scainolo/Zotero/storage/ZCWKG4EW/7973766.html}\n}\n\n@inproceedings{caino-loresMethodologicalApproachDataCentric2016,\n  title = {Methodological {{Approach}} to {{Data-Centric Cloudification}} of {{Scientific Iterative Workflows}}},\n  booktitle = {International {{Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},\n  author = {{Ca{\\'i}no-Lores}, Silvina and Lapin, Andrei and Kropf, Peter and Carretero, Jes{\\'u}s},\n  editor = {Carretero, Jesus and {Garcia-Blas}, Javier and Ko, Ryan K.L. and Mueller, Peter and Nakano, Koji},\n  year = {2016},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {469--482},\n  publisher = {Springer International Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-319-49583-5_36},\n  abstract = {The computational complexity and the constantly increasing amount of input data for scientific computing models is threatening their scalability. In addition, this is leading towards more data-intensive scientific computing, thus rising the need to combine techniques and infrastructures from the HPC and big data worlds. This paper presents a methodological approach to cloudify generalist iterative scientific workflows, with a focus on improving data locality and preserving performance. To evaluate this methodology, it was applied to an hydrological simulator, EnKF-HGS. The design was implemented using Apache Spark, and assessed in a local cluster and in Amazon Elastic Compute Cloud (EC2) against the original version to evaluate performance and scalability.},\n  isbn = {978-3-319-49583-5},\n  langid = {english},\n  keywords = {Apache spark,Cloud computing,Cloudification,Ensemble Kalman filter,HydroGeoSphere,Hydrology,Iterative workflows,Map reduce},\n  file = {/home/scainolo/Zotero/storage/2RCTKJ5I/Caíno-Lores et al. - 2016 - Methodological Approach to Data-Centric Cloudifica.pdf}\n}\n\n@inproceedings{caino-loresRuntimeSteeringMolecular2023a,\n  title = {Runtime {{Steering}} of {{Molecular Dynamics Simulations Through In Situ Analysis}} and {{Annotation}} of {{Collective Variables}}},\n  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},\n  author = {{Caino-Lores}, Silvina and Cuendet, Michel and Marquez, Jack and Kots, Ekaterina and Estrada, Trilce and Deelman, Ewa and Weinstein, Harel and Taufer, Michela},\n  year = {2023},\n  month = jun,\n  series = {{{PASC}} '23},\n  pages = {1--11},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3592979.3593420},\n  url = {https://doi.org/10.1145/3592979.3593420},\n  urldate = {2024-07-26},\n  abstract = {This paper targets one of the most common simulations on petascale and, very likely, on exascale machines: molecular dynamics (MD) simulations studying the (classical) time evolution of a molecular system at atomic resolution. Specifically, this work addresses the data challenges of MD simulations at exascale through (1) the creation of a data analysis method based on a suite of advanced collective variables (CVs) selected for annotation of structural molecular properties and capturing rare conformational events at runtime, (2) the definition of an in situ framework to automatically identify the frames where the rare events occur during an MD simulation and (3) the integration of both method and framework into two MD workflows for the study of early termination or termination and restart of a benchmark molecular system for protein folding ---the Fs peptide system (Ace-A\\_5(AAARA)\\_3A-NME)--- using Summit. The approach achieves faster exploration of the conformational space compared to extensive ensemble simulations. Specifically, our in situ framework with early termination alone achieves 99.6\\% coverage of the reference conformational space for the Fs peptide with just 60\\% of the MD steps otherwise used for a traditional execution of the MD simulation. Annotation-based restart allows us to cover 94.6\\% of the conformational space, just running 50\\% of the overall MD steps.},\n  isbn = {9798400701900},\n  file = {/home/scainolo/Zotero/storage/RDC6ECBC/Caino-Lores et al. - 2023 - Runtime Steering of Molecular Dynamics Simulations.pdf}\n}\n\n@inproceedings{caino-loresSparkDIYFrameworkInteroperable2018,\n  title = {Spark-{{DIY}}: {{A Framework}} for {{Interoperable Spark Operations}} with {{High Performance Block-Based Data Models}}},\n  shorttitle = {Spark-{{DIY}}},\n  booktitle = {2018 {{IEEE}}/{{ACM}} 5th {{International Conference}} on {{Big Data Computing Applications}} and {{Technologies}} ({{BDCAT}})},\n  author = {{Ca{\\'i}no-Lores}, Silvina and Carretero, Jes{\\'u}s and Nicolae, Bogdan and Yildiz, Orcun and Peterka, Tom},\n  year = {2018},\n  month = dec,\n  pages = {1--10},\n  doi = {10.1109/BDCAT.2018.00010},\n  abstract = {Today's scientific applications are increasingly relying on a variety of data sources, storage facilities, and computing infrastructures, and there is a growing demand for data analysis and visualization for these applications. In this context, exploiting Big Data frameworks for scientific computing is an opportunity to incorporate high-level libraries, platforms, and algorithms for machine learning, graph processing, and streaming; inherit their data awareness and fault-tolerance; and increase productivity. Nevertheless, limitations exist when Big Data platforms are integrated with an HPC environment, namely poor scalability, severe memory overhead, and huge development effort. This paper focuses on a popular Big Data framework -Apache Spark- and proposes an architecture to support the integration of highly scalable MPI block-based data models and communication patterns with a map-reduce-based programming model. The resulting platform preserves the data abstraction and programming interface of Spark, without conducting any changes in the framework, but allows the user to delegate operations to the MPI layer. The evaluation of our prototype shows that our approach integrates Spark and MPI efficiently at scale, so end users can take advantage of the productivity facilitated by the rich ecosystem of high-level Big Data tools and libraries based on Spark, without compromising efficiency and scalability.},\n  keywords = {Adaptation models,Big Data,Data models,HPC Big Data Spark MPI High-Performance analytics programming environments,Libraries,Programming,Sparks,Tools},\n  file = {/home/scainolo/Zotero/storage/Q2GCC36B/Caíno-Lores et al. - 2018 - Spark-DIY A Framework for Interoperable Spark Ope.pdf;/home/scainolo/Zotero/storage/6WVNN7E2/8606630.html}\n}\n\n@inproceedings{carreteroMultiObjectiveSimulatorOptimal2015a,\n  title = {A {{Multi-Objective Simulator}} for {{Optimal Power Dimensioning}} on {{Electric Railways}} Using {{Cloud Computing}}},\n  booktitle = {International {{Conference}} on {{Simulation}} and {{Modeling Methodologies}}, {{Technologies}} and {{Applications}}},\n  author = {Carretero, Jesus and Caino, Silvina and {Garcia-Carballeira}, Felix and Garcia, Alberto},\n  year = {2015},\n  month = jul,\n  volume = {2},\n  pages = {428--438},\n  publisher = {SCITEPRESS},\n  doi = {10.5220/0005573404280438},\n  url = {https://www.scitepress.org/PublishedPapers/2015/55734},\n  urldate = {2023-03-08},\n  abstract = {Power dimensioning and energy saving have been traditionally two main issues regarding the deployment of electric grids. Electric railways are also concerned about these issues, and simulators have been traditionally used to test such infrastructure deployments. The main goal of this paper is to present the Railway electric Power Consumption Simulator, a simulation model and tool for the railway energy provisioning problem. This simulator aims to propose electric railway infrastructure deployments, optimizing the quality of the electric flow supplied to train, as well as saving as much energy as possible. The paper describes the simulator structure, as well as the ontology used to translate railway infrastructure elements into an electric circuit. Because these two objectives are conflicting, a multi-objective optimization problem is formulated and solved. Finally, a standard railway scenario is used to illustrate the capabilities of the tool, trying to find the best electric substation placements in order to optimize such objectives. The evaluation shows how the tool can handle hundreds of simulated scenarios using Cloud Computing techniques.},\n  isbn = {978-989-758-120-5},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/E2KMAYQH/Carretero et al. - 2015 - A Multi-Objective Simulator for Optimal Power Dime.pdf}\n}\n\n@inproceedings{channingComposableWorkflowAccelerating2023,\n  title = {Composable {{Workflow}} for {{Accelerating Neural Architecture Search Using In Situ Analytics}} for {{Protein Classification}}},\n  booktitle = {Proceedings of the 52nd {{International Conference}} on {{Parallel Processing}}},\n  author = {Channing, Georgia and Patel, Ria and Olaya, Paula and Rorabaugh, Ariel and Miyashita, Osamu and {Caino-Lores}, Silvina and Schuman, Catherine and Tama, Florence and Taufer, Michela},\n  year = {2023},\n  month = sep,\n  series = {{{ICPP}} '23},\n  pages = {1},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3605573.3605636},\n  url = {https://doi.org/10.1145/3605573.3605636},\n  urldate = {2024-07-26},\n  abstract = {Neural architecture search (NAS), which automates the design of neural network (NN) architectures for scientific datasets, requires significant computational resources and time --- often on the order of days or weeks of GPU hours and training time. We design the Analytics for Neural Network (A4NN) workflow, a composable workflow that significantly reduces the time and resources required to design accurate and efficient NN architectures. We introduce a parametric fitness prediction strategy and distribute training across multiple accelerators to decrease the aggregated NN training time. A4NN rigorously record neural architecture histories, model states, and metadata to reproduce the search for near-optimal NNs. We demonstrate A4NN's ability to reduce training time and resource consumption on a dataset generated by an X-ray Free Electron Laser (XFEL) experiment simulation. When deploying A4NN, we decrease training time by up to 37\\% and epochs required by up to 38\\%.},\n  isbn = {9798400708435}\n}\n\n@inproceedings{olayaIdentifyingStructuralProperties2022,\n  title = {Identifying {{Structural Properties}} of {{Proteins}} from {{X-ray Free Electron Laser Diffraction Patterns}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Olaya, Paula and {Ca{\\'i}no-Lores}, Silvina and Lama, Vanessa and Patel, Ria and Rorabaugh, Ariel Keller and Miyashita, Osamu and Tama, Florence and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {21--31},\n  doi = {10.1109/eScience55777.2022.00017},\n  abstract = {Capturing structural information of a biological molecule is crucial to determine its function and understand its mechanics. X-ray Free Electron Lasers (XFEL) are an experimental method used to create diffraction patterns (images) that can reveal structural information. In this work we design, implement, and evaluate XPSI (X-ray Free Electron Laser-based Protein Structure Identifier), a framework capable of predicting three structural properties in molecules (i.e., orientation, conformation, and protein type) from their diffraction patterns. XPSI predicts these properties with high accuracy in challenging scenarios, such as recognizing orientations despite symmetries in diffraction patterns, distinguishing conformations even when they have similar structures, and identifying protein types under different noise conditions. Our framework shows low computational cost and high prediction accuracy compared to other machine learning methods such as random forest and neural networks.},\n  keywords = {autoencoder,Diffraction,diffraction patterns,Free electron lasers,Laser beams,machine learning,proteins,Proteins,Task analysis,X-ray diffraction,X-ray lasers},\n  file = {/home/scainolo/Zotero/storage/3C4ATLG5/Olaya et al. - 2022 - Identifying Structural Properties of Proteins from.pdf;/home/scainolo/Zotero/storage/L59I9EEQ/9973459.html}\n}\n\n@inproceedings{sahniOnlineBoostedGaussian2023c,\n  title = {Online {{Boosted Gaussian Learners}} for {{In-Situ Detection}} and {{Characterization}} of {{Protein Folding States}} in {{Molecular Dynamics Simulations}}},\n  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Sahni, Harshita and {Carrillo-Cabada}, Hector and Kots, Ekaterina and {Caino-Lores}, Silvina and Marquez, Jack and Deelman, Ewa and Cuendet, Michel and Weinstein, Harel and Taufer, Michela and Estrada, Trilce},\n  year = {2023},\n  month = oct,\n  pages = {1--10},\n  issn = {2325-3703},\n  doi = {10.1109/e-Science58273.2023.10254895},\n  url = {https://ieeexplore.ieee.org/abstract/document/10254895},\n  urldate = {2024-07-26},\n  abstract = {Molecular Dynamics (MD) simulations are a crucial tool for understanding how proteins fold. In its easiest form, MD simulations can be scaled through data parallelism, this means that multiple folding trajectories can be spawned and executed in parallel, facilitating a more efficient exploration of the protein folding space. However, due to data dependencies, the analysis of MD simulations remains largely as a centralized process. In this work, we propose a data parallel, lightweight technique to learn the characteristics of protein folding states in MD simulations. Contrary to other methods, ours can differentiate relevant states in a single protein folding trajectory without requiring centralized global knowledge of the protein dynamics. As its processing and memory overheads are negligible (in the order of milliseconds per window of frames, and kilo bytes respectively) this technique can be coupled with the simulation for in-situ analysis.},\n  keywords = {Analytical models,Data models,Parallel processing,Proteins,Trajectory},\n  file = {/home/scainolo/Zotero/storage/GJ8YQNIZ/10254895.html}\n}\n"},3342:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@article{afleReproducingResultsNeutron2023,\n  title = {Reproducing the {{Results}} for {{Neutron Star Interior Composition Explorer Observation}} of {{PSR J0030}} + 0451},\n  author = {Afle, Chaitanya and Miles, Patrick R. and {Ca{\\'i}no-Lores}, Silvina and Capano, Collin D. and Tews, Ingo and Vahi, Karan and Deelman, Ewa and Taufer, Michela and Brown, Duncan A.},\n  year = {2023},\n  month = nov,\n  journal = {Computing in Science \\& Engineering},\n  volume = {25},\n  number = {6},\n  pages = {16--26},\n  issn = {1558-366X},\n  doi = {10.1109/MCSE.2024.3381080},\n  url = {https://ieeexplore.ieee.org/abstract/document/10480447},\n  urldate = {2024-07-26},\n  abstract = {NASA's Neutron Star Interior Composition Explorer (NICER) observed X-ray emission from the pulsar PSR J0030 + 0451 in 2018. Riley et al. reported Bayesian parameter measurements of the mass and the star's radius using pulse-profile modeling of the X-ray data. This article reproduces their result using the open source software X-Ray Pulse Simulation and Inference and publicly available data within expected statistical errors. We note the challenges we faced in reproducing the results and demonstrate that the analysis can be reproduced and reused in future works by changing the prior distribution for the radius and the sampler configuration. We find no significant change in the measurement of the mass and radius, demonstrating that the original result is robust to these changes. Finally, we provide a containerized working environment that facilitates third-party reproduction of the measurements of the mass and radius of PSR J0030 + 0451 using the NICER observations.},\n  keywords = {Containers,Data models,Extraterrestrial measurements,Neutron stars,Reproducibility of results,Software,Temperature measurement},\n  file = {/home/scainolo/Zotero/storage/TMEGS844/Afle et al. - 2023 - Reproducing the Results for Neutron Star Interior .pdf;/home/scainolo/Zotero/storage/UM693AKK/10480447.html}\n}\n\n@article{caino-loresApplyingBigData2020a,\n  title = {Applying Big Data Paradigms to a Large Scale Scientific Workflow: {{Lessons}} Learned and Future Directions},\n  shorttitle = {Applying Big Data Paradigms to a Large Scale Scientific Workflow},\n  author = {{Caino-Lores}, S. and Lapin, A. and Carretero, J. and Kropf, P.},\n  year = {2020},\n  month = sep,\n  journal = {Future Generation Computer Systems},\n  volume = {110},\n  pages = {440--452},\n  issn = {0167-739X},\n  doi = {10.1016/j.future.2018.04.014},\n  url = {https://www.sciencedirect.com/science/article/pii/S0167739X16308214},\n  urldate = {2023-03-08},\n  abstract = {The increasing amounts of data related to the execution of scientific workflows has raised awareness of their shift towards parallel data-intensive problems. In this paper, we deliver our experience combining the traditional high-performance computing and grid-based approaches with Big Data analytics paradigms, in the context of scientific ensemble workflows. Our goal was to assess and discuss the suitability of such data-oriented mechanisms for production-ready workflows, especially in terms of scalability. We focused on two key elements in the Big Data ecosystem: the data-centric programming model, and the underlying infrastructure that integrates storage and computation in each node. We experimented with a representative MPI-based iterative workflow from the hydrology domain, EnKF-HGS, which we re-implemented using the Spark data analysis framework. We conducted experiments on a local cluster, a private cloud running OpenNebula, and the Amazon Elastic Compute Cloud (AmazonEC2). The results we obtained were analysed to synthesize the lessons we learned from this experience, while discussing promising directions for further research.},\n  langid = {english},\n  keywords = {Apache spark,Big data,Cloud computing,Hydrology,Scientific workflows},\n  file = {/home/scainolo/Zotero/storage/XJHGBGCV/Caíno-Lores et al. - 2020 - Applying big data paradigms to a large scale scien.pdf;/home/scainolo/Zotero/storage/L5QENQ85/S0167739X16308214.html}\n}\n\n@article{caino-loresCloudificationMethodologyMultidimensional2015a,\n  title = {A Cloudification Methodology for Multidimensional Analysis: {{Implementation}} and Application to a Railway Power Simulator},\n  shorttitle = {A Cloudification Methodology for Multidimensional Analysis},\n  author = {{Caino-Lores}, Silvina and Fern{\\'a}ndez, Alberto Garc{\\'i}a and {Garc{\\'i}a-Carballeira}, F{\\'e}lix and P{\\'e}rez, Jes{\\'u}s Carretero},\n  year = {2015},\n  month = jun,\n  journal = {Simulation Modelling Practice and Theory},\n  volume = {55},\n  pages = {46--62},\n  issn = {1569-190X},\n  doi = {10.1016/j.simpat.2015.04.002},\n  url = {https://www.sciencedirect.com/science/article/pii/S1569190X15000611},\n  urldate = {2023-03-08},\n  abstract = {Many scientific areas make extensive use of computer simulations to study complex real-world processes. These computations are typically very resource-intensive and present scalability issues as experiments get larger even in dedicated clusters, since these are limited by their own hardware resources. Cloud computing raises as an option to move forward into the ideal unlimited scalability by providing virtually infinite resources, yet applications must be adapted to this new paradigm. This process of converting and/or migrating an application and its data in order to make use of cloud computing is sometimes known as cloudifying the application. We propose a generalist cloudification method based in the MapReduce paradigm to migrate scientific simulations into the cloud to provide greater scalability. We analysed its viability by applying it to a real-world railway power consumption simulatior and running the resulting implementation on Hadoop YARN over Amazon EC2. Our tests show that the cloudified application is highly scalable and there is still a large margin to improve the theoretical model and its implementations, and also to extend it to a wider range of simulations. We also propose and evaluate a multidimensional analysis tool based on the cloudified application. It generates, executes and evaluates several experiments in parallel, for the same simulation kernel. The results we obtained indicate that out methodology is suitable for resource intensive simulations and multidimensional analysis, as it improves infrastructure's utilization, efficiency and scalability when running many complex experiments.},\n  langid = {english},\n  keywords = {Cloud computing,Many-task computing,MapReduce,Migration,Railway simulator},\n  file = {/home/scainolo/Zotero/storage/ENYF2ESG/Caíno-Lores et al. - 2015 - A cloudification methodology for multidimensional .pdf;/home/scainolo/Zotero/storage/CBP89LZW/S1569190X15000611.html}\n}\n\n@article{caino-loresEfficientDesignAssessment2017,\n  title = {Efficient Design Assessment in the Railway Electric Infrastructure Domain Using Cloud Computing},\n  author = {{Ca{\\'i}no-Lores}, Silvina and Garc{\\'i}a, Alberto and {Garc{\\'i}a-Carballeira}, F{\\'e}lix and Carretero, Jes{\\'u}s},\n  year = {2017},\n  month = jan,\n  journal = {Integrated Computer-Aided Engineering},\n  volume = {24},\n  number = {1},\n  pages = {57--72},\n  publisher = {IOS Press},\n  issn = {1069-2509},\n  doi = {10.3233/ICA-160532},\n  url = {https://content.iospress.com/articles/integrated-computer-aided-engineering/ica532},\n  urldate = {2023-03-08},\n  abstract = {Nowadays, railway infrastructure designers rely heavily on computer simulators and expert systems to model, analyze and evaluate potential deployments prior to their installation. This paper presents the railway power consumption simulator model (RPC},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/T2THIUIA/Caíno-Lores et al. - 2017 - Efficient design assessment in the railway electri.pdf}\n}\n\n@article{caino-loresHighPerformanceComputingBig2019,\n  title = {Toward {{High-Performance Computing}} and {{Big Data Analytics Convergence}}: {{The Case}} of {{Spark-DIY}}},\n  shorttitle = {Toward {{High-Performance Computing}} and {{Big Data Analytics Convergence}}},\n  author = {{Caino-Lores}, Silvina and Carretero, Jes{\\'u}s and Nicolae, Bogdan and Yildiz, Orcun and Peterka, Tom},\n  year = {2019},\n  journal = {IEEE Access},\n  volume = {7},\n  pages = {156929--156955},\n  issn = {2169-3536},\n  doi = {10.1109/ACCESS.2019.2949836},\n  abstract = {Convergence between high-performance computing (HPC) and big data analytics (BDA) is currently an established research area that has spawned new opportunities for unifying the platform layer and data abstractions in these ecosystems. This work presents an architectural model that enables the interoperability of established BDA and HPC execution models, reflecting the key design features that interest both the HPC and BDA communities, and including an abstract data collection and operational model that generates a unified interface for hybrid applications. This architecture can be implemented in different ways depending on the process- and data-centric platforms of choice and the mechanisms put in place to effectively meet the requirements of the architecture. The Spark-DIY platform is introduced in the paper as a prototype implementation of the architecture proposed. It preserves the interfaces and execution environment of the popular BDA platform Apache Spark, making it compatible with any Spark-based application and tool, while providing efficient communication and kernel execution via DIY, a powerful communication pattern library built on top of MPI. Later, Spark-DIY is analyzed in terms of performance by building a representative use case from the hydrogeology domain, EnKF-HGS. This application is a clear example of how current HPC simulations are evolving toward hybrid HPC-BDA applications, integrating HPC simulations within a BDA environment.},\n  keywords = {Analytical models,Big Data,Big data analytics,Business,Computer architecture,Convergence,Data models,Data visualization,DIY,high performance computing,MPI,spark},\n  file = {/home/scainolo/Zotero/storage/BPBTWXI7/Caíno-Lores et al. - 2019 - Toward High-Performance Computing and Big Data Ana.pdf;/home/scainolo/Zotero/storage/GRWI4NVG/8884083.html}\n}\n\n@article{caino-loresSurveyDataCentricDataAware2016,\n  title = {A {{Survey}} on {{Data-Centric}} and {{Data-Aware Techniques}} for {{Large Scale Infrastructures}}},\n  author = {{Caino-Lores}, Silvina and Carretero, J.},\n  year = {2016},\n  month = feb,\n  journal = {International Journal of Computer and Information Engineering},\n  doi = {10.5281/zenodo.1112258},\n  url = {https://www.semanticscholar.org/paper/A-Survey-on-Data-Centric-and-Data-Aware-Techniques-Ca%C3%ADno-Lores-Carretero/9905b6cdc281dced8d9a25375cac542453e38ef3},\n  urldate = {2023-03-08},\n  abstract = {Large scale computing infrastructures have been widely developed with the core objective of providing a suitable platform for high-performance and high-throughput computing. These systems are designed to support resource-intensive and complex applications, which can be found in many scientific and industrial areas. Currently, large scale data-intensive applications are hindered by the high latencies that result from the access to vastly distributed data. Recent works have suggested that improving data locality is key to move towards exascale infrastructures efficiently, as solutions to this problem aim to reduce the bandwidth consumed in data transfers, and the overheads that arise from them. There are several techniques that attempt to move computations closer to the data. In this survey we analyse the different mechanisms that have been proposed to provide data locality for large scale high-performance and high-throughput systems. This survey intends to assist scientific computing community in understanding the various technical aspects and strategies that have been reported in recent literature regarding data locality. As a result, we present an overview of locality-oriented techniques, which are grouped in four main categories: application development, task scheduling, in-memory computing and storage platforms. Finally, the authors include a discussion on future research lines and synergies among the former techniques. Keywords--- Co-scheduling, data-centric, data-intensive, data locality, in-memory storage, large scale.}\n}\n\n@article{doLightweightMethodEvaluating2021,\n  title = {A Lightweight Method for Evaluating in Situ Workflow Efficiency},\n  author = {Do, Tu Mai Anh and Pottier, Lo{\\\"i}c and {Caino-Lores}, Silvina and {Ferreira da Silva}, Rafael and Cuendet, Michel A. and Weinstein, Harel and Estrada, Trilce and Taufer, Michela and Deelman, Ewa},\n  year = {2021},\n  month = jan,\n  journal = {Journal of Computational Science},\n  volume = {48},\n  pages = {101259},\n  issn = {1877-7503},\n  doi = {10.1016/j.jocs.2020.101259},\n  url = {https://www.sciencedirect.com/science/article/pii/S1877750320305573},\n  urldate = {2023-03-08},\n  abstract = {Performance evaluation is crucial to understanding the behavior of scientific workflows. In this study, we target an emerging type of workflow, called in situ workflows. These workflows tightly couple components such as simulation and analysis to improve overall workflow performance. To understand the tradeoffs of various configurable parameters for coupling these heterogeneous tasks, namely simulation stride, and component placement, separately monitoring each component is insufficient to gain insights into the entire workflow behavior. Through an analysis of the state-of-the-art research, we propose a lightweight metric, derived from a defined in situ step, for assessing resource usage efficiency of an in situ workflow execution. By applying this metric to a synthetic workflow, which is parameterized to emulate behaviors of a molecular dynamics simulation, we explore two possible scenarios (Idle Simulation and Idle Analyzer) for the characterization of in situ workflow execution. In addition to preliminary results from a recently published study [11], we further exploit the proposed metric to evaluate a practical in situ workflow with a real molecular dynamics application, i.e., GROMACS. Experimental results show that the in transit placement (analytics on dedicated nodes) sustains a higher frequency for performing in situ analysis compared to the helper-core configuration (analytics co-allocated with simulation).},\n  langid = {english},\n  keywords = {High-performance computing,model,Molecular dynamics,Scientific workflow},\n  file = {/home/scainolo/Zotero/storage/D2B5BMXG/Do et al. - 2021 - A lightweight method for evaluating in situ workfl.pdf}\n}\n\n@article{doPerformanceAssessmentEnsembles2022,\n  title = {Performance Assessment of Ensembles of in Situ Workflows under Resource Constraints},\n  author = {Do, Tu Mai Anh and Pottier, Lo{\\\"i}c and {Ferreira da Silva}, Rafael and {Caino-Lores}, Silvina and Taufer, Michela and Deelman, Ewa},\n  year = {2022},\n  journal = {Concurrency and Computation: Practice and Experience},\n  volume = {n/a},\n  number = {n/a},\n  pages = {e7111},\n  issn = {1532-0634},\n  doi = {10.1002/cpe.7111},\n  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7111},\n  urldate = {2023-03-08},\n  abstract = {Scientific breakthroughs in biomolecular methods and improvements in hardware technology have shifted from a long-running simulation to a large set of shorter simulations running simultaneously, called an ensemble. In an ensemble, simulations are usually coupled with analyses of data produced by the simulations. In situ methods can be used to analyze large volumes of data generated by scientific simulations at runtime (i.e., simulations and analyses are performed concurrently). In this work, we study the execution of ensemble-based simulations paired with in situ analyses using in-memory staging methods. Using an ensemble of molecular dynamics in situ workflows with multiple simulations and analyses, we first show that collecting traditional metrics such as makespan, instructions per cycle, memory usage, or cache miss ratio is not sufficient to characterize complex behaviors of ensembles. We propose a method to evaluate the performance of ensembles of workflows that captures multiple resource usage aspects: resource efficiency, resource allocation, and resource provisioning. Experimental results demonstrate that the proposed method can effectively distinguish the performance of different component placements in an ensemble with up to 32 ensemble members. By evaluating different co-location scenarios, our proposed performance indicators demonstrate benefits of co-locating simulation and coupled analyses within a compute node.},\n  langid = {english},\n  keywords = {ensemble workflow,high-performance computing,in situ model,molecular dynamics,scientific workflow},\n  file = {/home/scainolo/Zotero/storage/5EX56JBF/Do et al. - Performance assessment of ensembles of in situ wor.pdf;/home/scainolo/Zotero/storage/3RVKBTXF/cpe.html}\n}\n\n@article{herbeinAnalyticalPerformanceModel2022a,\n  title = {An Analytical Performance Model of Generalized Hierarchical Scheduling},\n  author = {Herbein, Stephen and Patki, Tapasya and Ahn, Dong H and Mobo, Sebastian and Hathaway, Clark and {Caino-Lores}, Silvina and Corbett, James and Domyancic, David and Scogland, Thomas RW and {de Supinski}, Bronis R and Taufer, Michela},\n  year = {2022},\n  month = may,\n  journal = {The International Journal of High Performance Computing Applications},\n  volume = {36},\n  number = {3},\n  pages = {289--306},\n  publisher = {SAGE Publications Ltd STM},\n  issn = {1094-3420},\n  doi = {10.1177/10943420211051039},\n  url = {https://doi.org/10.1177/10943420211051039},\n  urldate = {2023-03-08},\n  abstract = {High performance computing (HPC) workflows are undergoing tumultuous changes, including an explosion in size and complexity. Despite these changes, most batch job systems still use slow, centralized schedulers. Generalized hierarchical scheduling (GHS) solves many of the challenges that face modern workflows, but GHS has not been widely adopted in HPC. A major difficulty that hinders adoption is the lack of a performance model to aid in configuring GHS for optimal performance on a given application. We propose an analytical performance model of GHS, and we validate our proposed model with four different applications on a moderately-sized system. Our validation shows that our model is extremely accurate at predicting the performance of GHS, explaining 98.7\\% of the variance (i.e., an R2 statistic of 0.987). Our results also support the claim that GHS overcomes scheduling throughput problems; we measured throughput improvements of up to 270? on our moderately-sized system. We then apply our performance model to a pre-exascale system, where our model predicts throughput improvements of four orders of magnitude and provides insight into optimally configuring GHS on next generation systems.},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/AT3G595Q/Herbein et al. - 2022 - An analytical performance model of generalized hie.pdf}\n}\n\n@article{kellerrorabaughBuildingHighThroughputNeural2022a,\n  title = {Building {{High-Throughput Neural Architecture Search Workflows}} via a {{Decoupled Fitness Prediction Engine}}},\n  author = {Keller Rorabaugh, Ariel and {Caino-Lores}, Silvina and Johnston, Travis and Taufer, Michela},\n  year = {2022},\n  month = nov,\n  journal = {IEEE Transactions on Parallel and Distributed Systems},\n  volume = {33},\n  number = {11},\n  pages = {2913--2926},\n  issn = {1558-2183},\n  doi = {10.1109/TPDS.2022.3140681},\n  abstract = {Neural networks (NN) are used in high-performance computing and high-throughput analysis to extract knowledge from datasets. Neural architecture search (NAS) automates NN design by generating, training, and analyzing thousands of NNs. However, NAS requires massive computational power for NN training. To address challenges of efficiency and scalability, we propose PENGUIN, a decoupled fitness prediction engine that informs the search without interfering in it. PENGUIN uses parametric modeling to predict fitness of NNs. Existing NAS methods and parametric modeling functions can be plugged into PENGUIN to build flexible NAS workflows. Through this decoupling and flexible parametric modeling, PENGUIN reduces training costs: it predicts the fitness of NNs, enabling NAS to terminate training NNs early. Early termination increases the number of NNs that fixed compute resources can evaluate, thus giving NAS additional opportunity to find better NNs. We assess the effectiveness of our engine on 6,000 NNs across three diverse benchmark datasets and three state of the art NAS implementations using the Summit supercomputer. Augmenting these NAS implementations with PENGUIN can increase throughput by a factor of 1.6 to 7.1. Furthermore, walltime tests indicate that PENGUIN can reduce training time by a factor of 2.5 to 5.3.},\n  keywords = {artificial intelligence,Artificial neural networks,Data models,Engines,Machine learning,neural networks,Parametric statistics,performance prediction,Predictive models,Search problems,Training},\n  file = {/home/scainolo/Zotero/storage/EX9JIG5V/Keller Rorabaugh et al. - 2022 - Building High-Throughput Neural Architecture Searc.pdf}\n}\n\n@article{patelReproducibilityFirstImage2022c,\n  title = {Reproducibility of the {{First Image}} of a {{Black Hole}} in the {{Galaxy M87 From}} the {{Event Horizon Telescope Collaboration}}},\n  author = {Patel, Ria and Roachell, Brandan and {Ca{\\'i}no-Lores}, Silvina and Ketron, Ross and Leonard, Jacob and Tan, Nigel and Vahi, Karan and Brown, Duncan A. and Deelman, Ewa and Taufer, Michela},\n  year = {2022},\n  month = sep,\n  journal = {Computing in Science \\& Engineering},\n  volume = {24},\n  number = {5},\n  pages = {42--52},\n  issn = {1558-366X},\n  doi = {10.1109/MCSE.2023.3241105},\n  url = {https://ieeexplore.ieee.org/abstract/document/10040660},\n  urldate = {2024-07-26},\n  abstract = {This article presents an interdisciplinary effort to develop and share sustainable knowledge necessary to analyze, understand, and use published scientific results to advance reproducibility in multimessenger astrophysics. Specifically, we target the breakthrough work associated with generating the first image of a black hole, called M87. The Event Horizon Telescope (EHT) Collaboration computed the image. Based on the artifacts made available by the EHT, we deliver documentation, code, and a computational environment to reproduce the first image of a black hole. Our deliverables support discovery in multimessenger astrophysics by providing all of the necessary tools for generalizing methods and findings from the EHT use case. Challenges encountered during the reproducibility of EHT results are reported. Our effort results in an open source, containerized software package that enables the public to reproduce the first image of a black hole in the galaxy M87.},\n  keywords = {Documentation,Image reconstruction,Imaging,Pipelines,Reproducibility of results,Software,Telescopes},\n  file = {/home/scainolo/Zotero/storage/MSWTCPLI/Patel et al. - 2022 - Reproducibility of the First Image of a Black Hole.pdf;/home/scainolo/Zotero/storage/5SA8J4FF/10040660.html}\n}\n\n@article{rorabaughHighFrequencyAccuracy2022a,\n  title = {High Frequency Accuracy and Loss Data of Random Neural Networks Trained on Image Datasets},\n  author = {Rorabaugh, Ariel Keller and {Ca{\\'i}no-Lores}, Silvina and Johnston, Travis and Taufer, Michela},\n  year = {2022},\n  month = feb,\n  journal = {Data in Brief},\n  volume = {40},\n  pages = {107780},\n  issn = {2352-3409},\n  doi = {10.1016/j.dib.2021.107780},\n  url = {https://www.sciencedirect.com/science/article/pii/S2352340921010544},\n  urldate = {2023-03-08},\n  abstract = {Neural Networks (NNs) are increasingly used across scientific domains to extract knowledge from experimental or computational data. An NN is composed of natural or artificial neurons that serve as simple processing units and are interconnected into a model architecture; it acquires knowledge from the environment through a learning process and stores this knowledge in its connections. The learning process is conducted by training. During NN training, the learning process can be tracked by periodically validating the NN and calculating its fitness. The resulting sequence of fitness values (i.e., validation accuracy or validation loss) is called the NN learning curve. The development of tools for NN design requires knowledge of diverse NNs and their complete learning curves. Generally, only final fully-trained fitness values for highly accurate NNs are made available to the community, hampering efforts to develop tools for NN design and leaving unaddressed aspects such as explaining the generation of an NN and reproducing its learning process. Our dataset fills this gap by fully recording the structure, metadata, and complete learning curves for a wide variety of random NNs throughout their training. Our dataset captures the lifespan of 6000 NNs throughout generation, training, and validation stages. It consists of a suite of 6000 tables, each table representing the lifespan of one NN. We generate each NN with randomized parameter values and train it for 40 epochs on one of three diverse image datasets (i.e., CIFAR-100, FashionMNIST, SVHN). We calculate and record each NN's fitness with high frequency---every half epoch---to capture the evolution of the training and validation process. As a result, for each NN, we record the generated parameter values describing the structure of that NN, the image dataset on which the NN trained, and all loss and accuracy values for the NN every half epoch. We put our dataset to the service of researchers studying NN performance and its evolution throughout training and validation. Statistical methods can be applied to our dataset to analyze the shape of learning curves in diverse NNs, and the relationship between an NN's structure and its fitness. Additionally, the structural data and metadata that we record enable the reconstruction and reproducibility of the associated NN.},\n  langid = {english},\n  keywords = {Accuracy curve,Artificial intelligence,Classification,Early stopping,Loss curve,Machine learning,Neural architecture search,Performance prediction},\n  file = {/home/scainolo/Zotero/storage/KKWBJQ2S/Rorabaugh et al. - 2022 - High frequency accuracy and loss data of random ne.pdf;/home/scainolo/Zotero/storage/RPR2YTWC/S2352340921010544.html}\n}\n"},7531:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@inproceedings{caino2014_sarteco,\n  title = {Breaking Data Dependences in Numerical Simulations Using Map-Reduce},\n  booktitle = {25th {{SARTECO}} Parallelism Meeting},\n  author = {{Caino-Lores, Silvina} and Garc{\\'i}a, Alberto and {Garc{\\'i}a-Carballeira}, F{\\'e}lix and Carretero, Jes{\\'u}s},\n  year = {2014},\n  month = sep,\n  address = {Valladolid, Spain},\n  keywords = {notion}\n}\n\n@inproceedings{caino2016_sarteco,\n  title = {Cloudification of a Legacy Hydrological Simulator Using Apache Spark},\n  booktitle = {27th {{SARTECO}} Parallelism Meeting},\n  author = {{Caino-Lores, Silvina} and Lapin, Andrei and Kropf, Peter and Carretero, Jes{\\'u}s},\n  year = {2016},\n  month = sep,\n  address = {Salamanca, Spain},\n  keywords = {notion}\n}\n"},8350:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@inproceedings{caino2022_escience,\n  title = {High-Throughput in-Situ Workflows for Ensemble Molecular Dynamics},\n  booktitle = {Proceedings of the 18th {{IEEE International Conference}} on E-{{Science}} ({{eScience}})},\n  author = {{Caino-Lores, S.} and Cuendet, M. and Estrada, T. and Deelman, E. and Weinstein, H. and {M. Taufer}},\n  year = {2022},\n  month = oct,\n  pages = {1--1},\n  publisher = {IEEE Computer Society},\n  address = {Salt Lake City, Utah, USA},\n  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9973469},\n  howpublished = {Proceedings of the 18th IEEE International Conference on e-Science (eScience)}\n}\n\n@inproceedings{ketronCaseStudyScientific2021a,\n  title = {A {{Case Study}} in {{Scientific Reproducibility}} from the {{Event Horizon Telescope}} ({{EHT}})},\n  booktitle = {2021 {{IEEE}} 17th {{International Conference}} on {{eScience}} ({{eScience}})},\n  author = {Ketron, R. and Leonard, J. and Roachell, B. and Patel, R. and White, R. and {Ca{\\'i}no-Lores}, S. and Tan, N. and Miles, P. and Vahi, K. and Deelman, E. and Brown, D. and Taufer, M.},\n  year = {2021},\n  month = sep,\n  pages = {249--250},\n  address = {Innsbruck, Austria},\n  doi = {10.1109/eScience51609.2021.00045},\n  abstract = {This poster presents the first results of an interdisciplinary project aiming to develop and share sustainable knowledge necessary to analyze, understand, and use published scientific results to advance reproducibility in multi-messenger astrophysics. Specifically, the project targets breakthrough work associated with the First M87 Event Horizon Telescope (EHT) and delivers recommendations on how the published results of the first black hole can be effectively reproduced. The project has the potential to advance new discovery in multi-messenger astrophysics by providing guidance for generalizing methods and findings from use cases.},\n  keywords = {Astrophysics,black hole image,Conferences,Multi-messenger astrophysics,Reproducibility of results,Telescopes},\n  file = {/home/scainolo/Zotero/storage/P7FHT7H5/Ketron et al. - 2021 - A Case Study in Scientific Reproducibility from th.pdf;/home/scainolo/Zotero/storage/FEB5X6Y9/9582335.html}\n}\n\n@inproceedings{olayaXPSIXrayFree2020,\n  title = {{{XPSI}}: {{X-ray Free Electron Laser-based Protein Structure Identifier}}},\n  booktitle = {Research {{Posters}} - {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}}, and {{Analysis}} ({{SC20}})},\n  author = {Olaya, Paula and Wyatt II, Michael R and {Ca{\\i}no-Lores}, Silvina and Tama, Florence},\n  year = {2020},\n  url = {https://sc20.supercomputing.org/proceedings/tech_poster/poster_files/rpost110s2-file3.pdf},\n  abstract = {A protein's structure determines its function. Different proteins have different structures; proteins in the same family share similar substructures and thus may share similar functions. Additionally, one protein may exhibit several structural states, also named conformations. Identifying different proteins and their conformations can help solve problems such as determining the cause of diseases and designing drugs. Xray Free Electron Laser (XFEL) beams are used to create diffraction patterns (images) that can reveal protein structure and function. The translation from diffraction patterns in the XFEL images to protein structures and functionalities is nontrivial. In this poster we present the first steps into the design and assessment of a software framework for the identification of XFEL images. We call the framework XPSI (XFEL-based Protein Structure Identifier). We quantify the identification accuracy and performance of XPSI for protein diffraction imaging datasets including different protein orientations and conformations with realistic noise incorporated.},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/AC9CEY24/Olaya et al. - XPSI X-ray Free Electron Laser-based Protein Stru.pdf;/home/scainolo/Zotero/storage/A3H8ISVB/rpost110.html}\n}\n\n@inproceedings{patelMethodologyGenerateEfficient2022a,\n  title = {A {{Methodology}} to {{Generate Efficient Neural Networks}} for {{Classification}} of {{Scientific Datasets}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Patel, Ria and Rorabaugh, Ariel Keller and Olaya, Paula and {Caino-Lores}, Silvina and Channing, Georgia and Schuman, Catherine and Miyashita, Osamu and Tama, Florence and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {389--390},\n  address = {Salt Lake City, Utah, USA},\n  doi = {10.1109/eScience55777.2022.00052},\n  abstract = {Neural networks (NNs) are increasingly utilized in high-throughput scientific workflows. In this context, NN efficiency is essential for successful workflow management. We use a multi-objective Neural Architecture Search (NAS), NSGA-Net, to search for highly accurate NNs while optimizing for efficient use of computational resources by minimizing FLoating-point Operations Per Second (FLOPS). We define a domain-agnostic methodology to generate NNs with the support of NSGA-Net, select promising NNs that balance accuracy and FLOPS usage, and refine a subset of NNs in order to curate networks suitable for efficient data analysis. We apply this methodology to a protein diffraction use case. Preliminary results show NNs that efficiently classify conformation of proteins with a final accuracy of 97.7\\% or higher and using only 187 FLOPS.},\n  keywords = {Artificial neural networks,Computational efficiency,Computer architecture,Data analysis,Diffraction,NAS,neural architecture search,NSGA-Net,Proteins},\n  file = {/home/scainolo/Zotero/storage/XWZ5IYV8/Patel et al. - 2022 - A Methodology to Generate Efficient Neural Network.pdf;/home/scainolo/Zotero/storage/D6GIS5JD/9973637.html}\n}\n\n@inproceedings{yeomUbiqueNewModel2022a,\n  title = {Ubique: {{A New Model}} for {{Untangling Inter-task Data Dependence}} in {{Complex HPC Workflows}}},\n  shorttitle = {Ubique},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Yeom, Jae-Seung and Ahn, Dong H. and Lumsden, Ian and Luettgau, Jakob and {Caino-Lores}, Silvina and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {421--422},\n  address = {Salt Lake City, Utah, USA},\n  doi = {10.1109/eScience55777.2022.00068},\n  abstract = {Exploiting task parallelism is getting increasingly difficult for diverse and complex scientific workflows running on High Performance Computing (HPC) systems. In this paper, we argue that the difficulty rises from a void in the spectrum of existing data-transfer models for resolving inter-task data dependence within a workflow and propose a novel model to fill that gap: Ubique. The Ubique model combines the best from in-transit and in situ models in order for loosely coupled producer and consumer tasks to run concurrently and to resolve their data dependencies efficiently with little or no modifications to their codes, striking a balance between transparent optimization, productivity, and performance. Our preliminary evaluation suggests that Ubique can significantly outperform the parallel file system (PFS)-based model while offering automatic data transfer and synchronization which are the features lacking in many traditional models. It also identifies the performance characteristics of its key depending subsystems, which must be understood for further broadening its benefits.},\n  keywords = {Computational modeling,Data models,Data transfer,File systems,High performance computing,in situ,in transit,job scheduling,Parallel processing,Productivity,workflow},\n  file = {/home/scainolo/Zotero/storage/5QSKKNCV/Yeom et al. - 2022 - Ubique A New Model for Untangling Inter-task Data.pdf;/home/scainolo/Zotero/storage/JD48ZTJ3/9973591.html}\n}\n"},8748:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@misc{rorabaughPEng4NNAccuratePerformance2021,\n  title = {{{PEng4NN}}: {{An Accurate Performance Estimation Engine}} for {{Efficient Automated Neural Network Architecture Search}}},\n  shorttitle = {{{PEng4NN}}},\n  author = {Rorabaugh, Ariel Keller and {Ca{\\'i}no-Lores}, Silvina and Wyatt II, Michael R. and Johnston, Travis and Taufer, Michela},\n  year = {2021},\n  month = aug,\n  number = {arXiv:2101.04185},\n  eprint = {2101.04185},\n  primaryclass = {cs},\n  publisher = {arXiv},\n  doi = {10.48550/arXiv.2101.04185},\n  url = {http://arxiv.org/abs/2101.04185},\n  urldate = {2024-07-26},\n  abstract = {Neural network (NN) models are increasingly used in scientific simulations, AI, and other high performance computing (HPC) fields to extract knowledge from datasets. Each dataset requires tailored NN model architecture, but designing structures by hand is a time-consuming and error-prone process. Neural architecture search (NAS) automates the design of NN architectures. NAS attempts to find well-performing NN models for specialized datsets, where performance is measured by key metrics that capture the NN capabilities (e.g., accuracy of classification of samples in a dataset). Existing NAS methods are resource intensive, especially when searching for highly accurate models for larger and larger datasets. To address this problem, we propose a performance estimation strategy that reduces the resources for training NNs and increases NAS throughput without jeopardizing accuracy. We implement our strategy via an engine called PEng4NN that plugs into existing NAS methods; in doing so, PEng4NN predicts the final accuracy of NNs early in the training process, informs the NAS of NN performance, and thus enables the NAS to terminate training NNs early. We assess our engine on three diverse datasets (i.e., CIFAR-100, Fashion MNIST, and SVHN). By reducing the training epochs needed, our engine achieves substantial throughput gain; on average, our engine saves 61\\% to 82\\% of training epochs, increasing throughput by a factor of 2.5 to 5 compared to a state-of-the-art NAS method. We achieve this gain without compromising accuracy, as we demonstrate with two key outcomes. First, across all our tests, between 74\\% and 97\\% of the ground truth best models lie in our set of predicted best models. Second, the accuracy distributions of the ground truth best models and our predicted best models are comparable, with the mean accuracy values differing by at most .7 percentage points across all tests.},\n  archiveprefix = {arXiv},\n  keywords = {Computer Science - Machine Learning,I.2.8},\n  file = {/home/scainolo/Zotero/storage/IWU287W3/Rorabaugh et al. - 2021 - PEng4NN An Accurate Performance Estimation Engine.pdf;/home/scainolo/Zotero/storage/QXGPTNPQ/2101.html}\n}\n"},2725:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@techreport{caino2017_report,\n  title = {Report on Application Adaptation and Deployment},\n  author = {{Caino-Lores, Silvina} and Serrano, Estefania and Garcia, Alberto and Sotomayor, Rafael and Sanchez, Luis Miguel},\n  year = {2017}\n}\n\n@techreport{caino2017technical,\n  title = {Technical Report on Techniques to Expose and Exploit Data Locality},\n  author = {{Caino-Lores, Silvina} and Duro, Francisco Jose Rodrigo and Garcia, Francisco Javier and Carretero, Jesus},\n  year = {2017},\n  url = {https://www.arcos.inf.uc3m.es/wp-content/uploads/sites/47/2017/02/wg4-E4.1-v1.0.pdf}\n}\n\n@techreport{caino2018_report,\n  title = {Report on Techniques for Data Management on Integrated {{HPC}} and {{Big Data}} Platforms},\n  author = {Carretero, Jesus and {Garcia-Carballeira}, Felix and Garcia, Francisco Javier and Isaila, Florin and {Caino-Lores}, Silvina and Serrano, Estefania},\n  year = {2018},\n  url = {https://www.arcos.inf.uc3m.es/bighpc/wp-content/uploads/sites/52/2018/09/d-2.2.pdf}\n}\n\n@techreport{dasilvaWorkflowsCommunitySummit2021,\n  title = {Workflows {{Community Summit}}: {{Advancing}} the {{State-of-the-art}} of {{Scientific Workflows Management Systems Research}} and {{Development}}},\n  shorttitle = {Workflows {{Community Summit}}},\n  author = {{da Silva}, Rafael Ferreira and Casanova, Henri and Chard, Kyle and Coleman, Tain{\\~a} and Laney, Dan and Ahn, Dong and Jha, Shantenu and Howell, Dorran and {Soiland-Reys}, Stian and Altintas, Ilkay and Thain, Douglas and Filgueira, Rosa and Babuji, Yadu and Badia, Rosa M. and Balis, Bartosz and {Caino-Lores}, Silvina and Callaghan, Scott and Coppens, Frederik and Crusoe, Michael R. and De, Kaushik and Di Natale, Frank and Do, Tu M. A. and Enders, Bjoern and Fahringer, Thomas and Fouilloux, Anne and Fursin, Grigori and Gaignard, Alban and Ganose, Alex and Garijo, Daniel and Gesing, Sandra and Goble, Carole and Hasan, Adil and Huber, Sebastiaan and Katz, Daniel S. and Leser, Ulf and Lowe, Douglas and Ludaescher, Bertram and Maheshwari, Ketan and Malawski, Maciej and Mayani, Rajiv and Mehta, Kshitij and Merzky, Andre and Munson, Todd and Ozik, Jonathan and Pottier, Lo{\\\"i}c and Ristov, Sashko and Roozmeh, Mehdi and Souza, Renan and Suter, Fr{\\'e}d{\\'e}ric and Tovar, Benjamin and Turilli, Matteo and Vahi, Karan and {Vidal-Torreira}, Alvaro and Whitcup, Wendy and Wilde, Michael and Williams, Alan and Wolf, Matthew and Wozniak, Justin},\n  year = {2021},\n  month = jun,\n  eprint = {2106.05177},\n  primaryclass = {cs},\n  doi = {10.5281/zenodo.4915801},\n  url = {http://arxiv.org/abs/2106.05177},\n  urldate = {2024-07-26},\n  abstract = {Scientific workflows are a cornerstone of modern scientific computing, and they have underpinned some of the most significant discoveries of the last decade. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale HPC platforms. Workflows will play a crucial role in the data-oriented and post-Moore's computing landscape as they democratize the application of cutting-edge research techniques, computationally intensive methods, and use of new computing platforms. As workflows continue to be adopted by scientific projects and user communities, they are becoming more complex. Workflows are increasingly composed of tasks that perform computations such as short machine learning inference, multi-node simulations, long-running machine learning model training, amongst others, and thus increasingly rely on heterogeneous architectures that include CPUs but also GPUs and accelerators. The workflow management system (WMS) technology landscape is currently segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. Another fundamental problem is that there are conflicting theoretical bases and abstractions for a WMS. Systems that use the same underlying abstractions can likely be translated between, which is not the case for systems that use different abstractions. More information: https://workflowsri.org/summits/technical},\n  archiveprefix = {arXiv},\n  keywords = {Computer Science - Distributed Parallel and Cluster Computing},\n  file = {/home/scainolo/Zotero/storage/VUPNP9VZ/da Silva et al. - 2021 - Workflows Community Summit Advancing the State-of.pdf;/home/scainolo/Zotero/storage/FLA4KTQ6/2106.html}\n}\n\n@techreport{ferreiradasilvaWorkflowsCommunitySummit2023,\n  title = {Workflows {{Community Summit}} 2022: {{A Roadmap Revolution}}},\n  shorttitle = {Workflows {{Community Summit}} 2022},\n  author = {Ferreira Da Silva, Rafael and Badia, Rosa and Bala, Venkat and Bard, Deborah and Bremer, Peer-Timo and Buckley, Ian and {Caino-Lores}, Silvina and Chard, Kyle and Goble, Carole and Jha, Shantenu and Katz, Daniel S. and Laney, Daniel and Parashar, Manish and Suter, Fred and Tyler, Nick and Uram, Thomas and Altintas, Ilkay and Andersson, Stefan and Arndt, William and Aznar, Juan and Bader, Jonathan and Balis, Bartosz and Blanton, Christopher and Braghetto, Kelly and Brodutch, Aharon and Brunk, Paul and Casanova, Henri and Lierta, Alba and Chigu, Justin and Coleman, Taina and Collier, Nick and Colonnelli, Iacopo and Coppens, Frederik and Crusoe, Michael and Cunningham, Will and Kinoshita, Bruno and Di Tomasso, Paolo and Doutriaux, Charles and Downton, Matthew and Elwasif, Wael and Enders, Bjoern and Erdmann, Christopher and Fahringer, Thomas and Figueiredo, Ludmilla and Filgueira, Rosa and Foltin, Martin and Fouilloux, Anne and Gadelha, Luiz and Gallo, Andy and Garcia, Artur and Garijo, Daniel and Gerlach, Roman and Grant, Ryan E. and Grayson, Samuel and Grubel, Patricia and Gustafsson, Johan and Hayot, Valerie and Hernandez Mendoza, Oscar and Hilbrich, Marcus and Justine, Annmary and Laflotte, Ian and Lehmann, Fabian and Luckow, Andre and Luettgau, Jakob and Maheshwari, Ketan and Matsuda, Motohiko and Medic, Doriana and Mendygral, Pete and Michalewicz, Marek and Nonaka, Jorji and Pawlik, Maciej and Pottier, Loic and Pouchard, Line and Putz, Mathias and Radha, Santosh and Ramakrishnan, Lavanya and Ristov, Sashko and Romano, Paul and Rosendo, Daniel and Ruefenacht, Martin and Rycerz, Katarzyna and Saurabh, Nishant and Savchenko, Volodymyr and Schulz, Martin and Simpson, Christine and Sirvent, Raul and Skluzacek, Tyler and Reyes, Stian and Santos Souza, Renan and Sukumar, Sreenivas R. and Sun, Ziheng and Sussman, Alan and Thain, Douglas and Titov, Mikhail and Tovar, Benjamin and Tripathy, Aalap and Turilli, Matteo and Tuznik, Bartosz and {van Dam}, Hubertus and Vivas, Aurelio and Ward, Logan and Widener, Patrick and Wilkinson, Sean and Zawalska, Justyna and Zulfiqar, Mahnoor},\n  year = {2023},\n  month = mar,\n  number = {ORNL/TM-2023/2885},\n  institution = {Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States)},\n  doi = {10.2172/2006942},\n  url = {https://www.osti.gov/biblio/2006942},\n  urldate = {2024-07-26},\n  abstract = {Scientific workflows have become integral tools in broad scientific computing use cases. Science discovery is increasingly dependent on workflows to orchestrate large and complex scientific experiments that range from the execution of a cloud-based data preprocessing pipeline to multi-facility instrument-to-edge-to-HPC computational workflows. Given the changing landscape of scientific computing (often referred to as a computing continuum) and the evolving needs of emerging scientific applications, it is paramount that the development of novel scientific workflows and system functionalities seek to increase the efficiency, resilience, and pervasiveness of existing systems and applications. Specifically, the proliferation of machine learning/artificial intelligence (ML/AI) workflows, need for processing large-scale datasets produced by instruments at the edge, intensification of near real-time data processing, support for long-term experiment campaigns, and emergence of quantum computing as an adjunct to HPC, have significantly changed the functional and operational requirements of workflow systems. Workflow systems now need to, for example, support data streams from the edge-to-cloud-to-HPC, enable the management of many small-sized files, allow data reduction while ensuring high accuracy, orchestrate distributed services (workflows, instruments, data movement, provenance, publication, etc.) across computing and user facilities, among others. Further, to accelerate science, it is also necessary that these systems implement specifications/standards and APIs for seamless (horizontal and vertical) integration between systems and applications, as well as enable the publication of workflows and their associated products according to the FAIR principles.},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/U933SNJ3/Ferreira Da Silva et al. - 2023 - Workflows Community Summit 2022 A Roadmap Revolut.pdf}\n}\n"},9818:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@phdthesis{cainoloresAdaptationDeploymentEvaluation2014,\n  title = {Adaptation, Deployment and Evaluation of a Railway Simulator in Cloud Environments},\n  author = {Ca{\\'i}no Lores, Silvina},\n  year = {2014},\n  month = jun,\n  url = {https://hdl.handle.net/10016/26192},\n  urldate = {2024-07-26},\n  abstract = {Many scientific areas make extensive use of computer simulations to study realworld processes. As they become more complex and resource-intensive, traditional programming paradigms running on supercomputers have shown to be limited by their hardware resources. The Cloud and its elastic nature has been increasingly seen as a valid alternative for simulation execution, as it aims to provide virtually infinite resources, thus unlimited scalability. In order to bene t from this, simulators must be adapted to this paradigm since cloud migration tends to add virtualization and communication overhead. This work has the main objective of migrating a power consumption railway simulator to the Cloud, with minimal impact in the original code and preserving performance. We propose a data-centric adaptation based in MapReduce to distribute the simulation load across several nodes while minimising data transmission. We deployed our solution on an Amazon EC2 virtual cluster and measured its performance. We did the same in in our local cluster to compare the solution's performance against the original application when the Cloud's overhead is not present. Our tests show that the resulting application is highly scalable and shows a better overall performance regarding the original simulator in both environments. This document summarises the author's work during the whole adaptation development process .},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/G4YUKNB3/Caíno Lores - 2014 - Adaptation, deployment and evaluation of a railway.pdf}\n}\n\n@phdthesis{cainoloresConvergenceBigData2019,\n  title = {On the Convergence of Big Data Analytics and High-Performance Computing: A Novel Approach for Runtime Interoperability},\n  shorttitle = {On the Convergence of Big Data Analytics and High-Performance Computing},\n  author = {Caino Lores, Silvina},\n  year = {2019},\n  month = may,\n  url = {https://hdl.handle.net/10016/29720},\n  urldate = {2024-07-26},\n  abstract = {Convergence between high-performance computing (HPC) and Big Data analytics (BDA) is currently an established research area that spawned new opportunities for unifying the platformlayer and data abstractions in these ecosystems. This thesis builds on the hypothesis that HPC-BDA convergence at platform level can be attained by enabling runtime interoperability in a way that preserves BDA platform usability and productivity, exploits HPC scalability and performance, and expands both BDA and HPC capabilities to cope with prospect hybrid application models. The goal is to architect an abstract system that enables the interoperability of established BDA and HPC runtimes. In order to exploit the benefits of BDA data-centric paradigms, this thesis presents a data-centric transformation methodology to allow process-centric workloads the interaction with BDA platforms and storage infrastructures. Furthermore, an architecture to achieve full runtime interoperability is proposed. It reflects the key design features that interest both the HPC and BDA communities, and includes an abstract data collection and operational model that generates a unified interface for hybrid applications. It also incorporates a mechanism to transfer each stage of the application to the appropriate runtime. This architecture can be implemented in different ways depending on the process- and data-centric runtimes of choice, and the mechanisms put in place to effectively meet the requirements of the architecture. The Spark-DIY platformis introduced as a possible implementation. It preserves the interfaces and execution environment of the popular BDA platformApache Spark --thus making it compatible with any Spark-based application and tool-- while providing efficient communication and kernel execution via DIY, a powerful communication pattern library built on top of MPI. Finally, these solutions are analysed in terms of performance by applying them to a representative use case, EnKF-HGS. This application is a clear example of how current HPC simulations are evolving towards hybrid HPC-BDA applications, integrating HPC simulations within a BDA environment. Other auxiliary use cases --like an application from the railway domain and a BDA benchmark operator-- are also introduced to support other specific contributions of this thesis.},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/TCFAWZDS/Caino Lores - 2019 - On the convergence of big data analytics and high-.pdf}\n}\n\n@phdthesis{cainoloresEnablingDataLocality2015,\n  title = {Enabling Data Locality in Multidimensional Scientific Applications with Many-Task Computing and Map-Reduce},\n  author = {Ca{\\'i}no Lores, Silvina},\n  year = {2015},\n  month = sep,\n  langid = {english}\n}\n"},2169:(e,a,n)=>{n.d(a,{Z:()=>i});const i="@inproceedings{baderNovelApproachesScalable2023a,\n  title = {Novel {{Approaches Toward Scalable Composable Workflows}} in {{Hyper-Heterogeneous Computing Environments}}},\n  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},\n  author = {Bader, Jonathan and Belak, Jim and Bement, Matthew and Berry, Matthew and Carson, Robert and Cassol, Daniela and Chan, Stephen and Coleman, John and Day, Kastan and Duque, Alejandro and Fagnan, Kjiersten and Froula, Jeff and Jha, Shantenu and Katz, Daniel S. and Kica, Piotr and Kindratenko, Volodymyr and Kirton, Edward and Kothadia, Ramani and Laney, Daniel and Lehmann, Fabian and Leser, Ulf and Licho{\\l}ai, Sabina and Malawski, Maciej and Melara, Mario and Player, Elais and Rolchigo, Matt and Sarrafan, Setareh and Sul, Seung-Jin and Syed, Abdullah and Thamsen, Lauritz and Titov, Mikhail and Turilli, Matteo and {Caino-Lores}, Silvina and Mandal, Anirban},\n  year = {2023},\n  month = nov,\n  series = {{{SC-W}} '23},\n  pages = {2097--2108},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3624062.3626283},\n  url = {https://doi.org/10.1145/3624062.3626283},\n  urldate = {2024-07-26},\n  abstract = {The annual Workshop on Workflows in Support of Large-Scale Science (WORKS) is a premier venue for the scientific workflow community to present the latest advances in research and development on the many facets of scientific workflows throughout their life-cycle. The Lightning Talks at WORKS focus on describing a novel tool, scientific workflow, or concept, which are work-in-progress and address emerging technologies and frameworks to foster discussion in the community. This paper summarizes the lightning talks at the 2023 edition of WORKS, covering five topics: leveraging large language models to build and execute workflows; developing a common workflow scheduler interface; scaling uncertainty workflow applications on exascale computing systems; evaluating a transcriptomics workflow for cloud vs. HPC systems; and best practices in migrating legacy workflows to workflow management systems.},\n  isbn = {9798400707858},\n  file = {/home/scainolo/Zotero/storage/DGLVNBNK/Bader et al. - 2023 - Novel Approaches Toward Scalable Composable Workfl.pdf}\n}\n\n@inproceedings{caino-loresCloudificationMethodologyNumerical2014,\n  title = {A {{Cloudification Methodology}} for {{Numerical Simulations}}},\n  booktitle = {Euro-{{Par}} 2014: {{Parallel Processing Workshops}}},\n  author = {{Caino-Lores}, Silvina and Garc{\\'i}a, Alberto and {Garc{\\'i}a-Carballeira}, F{\\'e}lix and Carretero, Jes{\\'u}s},\n  editor = {Lopes, Lu{\\'i}s and {\\v Z}ilinskas, Julius and Costan, Alexandru and Cascella, Roberto G. and Kecskemeti, Gabor and Jeannot, Emmanuel and Cannataro, Mario and Ricci, Laura and Benkner, Siegfried and Petit, Salvador and Scarano, Vittorio and Gracia, Jos{\\'e} and Hunold, Sascha and Scott, Stephen L. and Lankes, Stefan and Lengauer, Christian and Carretero, Jes{\\'u}s and Breitbart, Jens and Alexander, Michael},\n  year = {2014},\n  series = {Lecture {{Notes}} in {{Computer Science}}},\n  pages = {375--386},\n  publisher = {Springer International Publishing},\n  address = {Cham},\n  doi = {10.1007/978-3-319-14313-2_32},\n  abstract = {Many scientific areas make extensive use of computer simulations to study complex real-world processes. These computations are typically very resource-intensive and present scalability issues as experiments get larger, even in dedicated clusters since they are limited by their own hardware resources. Cloud computing raises as an option to move forward into the ideal unlimited scalability by providing virtually infinite resources, yet applications must be adapted to this new paradigm. We propose a generalist cloudification method based in the MapReduce paradigm to migrate numerical simulations into the cloud to provide greater scalability. We analysed its viability by applying it to a real-world simulation and running the resulting implementation on Hadoop YARN over Amazons EC2. Our tests show that the cloudified application is highly scalable and there is still a large margin to improve the theoretical model and its implementations, and also to extend it to a wider range of simulations.},\n  isbn = {978-3-319-14313-2},\n  langid = {english},\n  keywords = {Cloud Computing,Hadoop MapReduce,Kernel Execution,MapReduce Framework,Overhead Line},\n  file = {/home/scainolo/Zotero/storage/QLUMEM7T/Caíno-Lores et al. - 2014 - A Cloudification Methodology for Numerical Simulat.pdf}\n}\n\n@inproceedings{caino2016_works,\n  title = {Lessons Learned from Applying Big Data Paradigms to a Large Scale Scientific Workflow},\n  booktitle = {11th Workshop on Workflows in Support of Large-Scale Science ({{WORKS}} 2016)},\n  author = {{Caino-Lores, Silvina} and Lapin, Andrei and Kropf, Peter and Carretero, Jes{\\'u}s},\n  year = {2016},\n  month = nov,\n  address = {Salt Lake City, Utah, USA},\n  url = {https://ceur-ws.org/Vol-1800/short1.pdf}\n}\n\n@inproceedings{doAssessingResourceProvisioning2021,\n  title = {Assessing {{Resource Provisioning}} and {{Allocation}} of {{Ensembles}} of {{In Situ Workflows}}},\n  booktitle = {50th {{International Conference}} on {{Parallel Processing Workshop}}},\n  author = {Do, Tu Mai Anh and Pottier, Lo{\\\"i}c and {Ferreira da Silva}, Rafael and {Ca{\\'i}no-Lores}, Silvina and Taufer, Michela and Deelman, Ewa},\n  year = {2021},\n  month = sep,\n  series = {{{ICPP Workshops}} '21},\n  pages = {1--10},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  doi = {10.1145/3458744.3474051},\n  url = {https://doi.org/10.1145/3458744.3474051},\n  urldate = {2023-03-08},\n  abstract = {Scientific breakthroughs in biomolecular methods and improvements in hardware technology have shifted from a single long-running simulation to a large set of shorter simulations running simultaneously, called an ensemble. In an ensemble, each independent simulation is usually coupled with several analyses that apply identical or distinct algorithms on data produced by the corresponding simulation. Today, In situ methods are used to analyze large volumes of data generated by scientific simulations at runtime. This work studies the execution of ensemble-based simulations paired with In situ analyses using in-memory staging methods. Because simulations and analyses forming an ensemble typically run concurrently, deploying an ensemble requires efficient co-location-aware strategies, making sure the data flow between simulations and analyses that form an In situ workflow is efficient. Using an ensemble of molecular dynamics In situ workflows with multiple simulations and analyses, we first show that collecting traditional metrics such as makespan, instructions per cycle, memory usage, or cache miss ratio is not sufficient to characterize the complex behaviors of ensembles. Thus, we propose a method to evaluate the performance of ensembles of workflows that captures resource usage (efficiency), resource allocation, and component placement. Experimental results demonstrate that our proposed method can effectively capture the performance of different component placements in an ensemble. By evaluating different co-location scenarios, our performance indicator demonstrates improvements of up to four orders of magnitude when co-locating simulation and coupled analyses within a single computational host.},\n  isbn = {978-1-4503-8441-4},\n  keywords = {Ensemble workflow,High-performance computing,Insitu model,Molecular dynamics,Scientific workflow},\n  file = {/home/scainolo/Zotero/storage/KPLC5GTA/Do et al. - 2021 - Assessing Resource Provisioning and Allocation of .pdf}\n}\n\n@inproceedings{doCoschedulingEnsemblesSitu2022,\n  title = {Co-Scheduling {{Ensembles}} of {{In Situ Workflows}}},\n  booktitle = {2022 {{IEEE}}/{{ACM Workshop}} on {{Workflows}} in {{Support}} of {{Large-Scale Science}} ({{WORKS}})},\n  author = {Do, Tu Mai Anh and Pottier, Lo{\\\"i}c and {da Silva}, Rafael Ferreira and Suter, Fr{\\'e}d{\\'e}ric and {Ca{\\'i}no-Lores}, Silvina and Taufer, Michela and Deelman, Ewa},\n  year = {2022},\n  month = nov,\n  pages = {43--51},\n  doi = {10.1109/WORKS56498.2022.00011},\n  url = {https://ieeexplore.ieee.org/abstract/document/10023936},\n  urldate = {2024-07-26},\n  abstract = {Molecular dynamics (MD) simulations are widely used to study large-scale molecular systems. HPC systems are ideal platforms to run these studies, however, reaching the necessary simulation timescale to detect rare processes is challenging, even with modern supercomputers. To overcome the timescale limitation, the simulation of a long MD trajectory is replaced by multiple short-range simulations that are executed simultaneously in an ensemble of simulations. Analyses are usually co-scheduled with these simulations to efficiently process large volumes of data generated by the simulations at runtime, thanks to in situ techniques. Executing a workflow ensemble of simulations and their in situ analyses requires efficient co-scheduling strategies and sophisticated management of computational resources so that they are not slowing down each other. In this paper, we propose an efficient method to co-schedule simulations and in situ analyses such that the makespan of the workflow ensemble is minimized. We present a novel approach to allocate resources for a workflow ensemble under resource constraints by using a theoretical framework modeling the workflow ensemble's execution. We evaluate the proposed approach using an accurate simulator based on the WRENCH simulation framework on various workflow ensemble configurations. Results demonstrate the significance of co-scheduling simulations and in situ analyses that couple data together to benefit from data locality, in which inefficient scheduling decisions can lead to slowdown in makespan up to a factor of 30.},\n  keywords = {Analytical models,co-scheduling,Computational modeling,Conferences,Data models,High performance computing,high-performance computing,in situ,molecular dynamics,Processor scheduling,Runtime,workflow ensemble},\n  file = {/home/scainolo/Zotero/storage/5LTNE4UT/Do et al. - 2022 - Co-scheduling Ensembles of In Situ Workflows.pdf;/home/scainolo/Zotero/storage/EF5QLWKD/10023936.html}\n}\n\n@inproceedings{luettgauReproducingExtendingAnalytical2022,\n  title = {Reproducing and {{Extending Analytical Performance Models}} of {{Generalized Hierarchical Scheduling}}},\n  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on E-{{Science}} (e-{{Science}})},\n  author = {Luettgau, Jakob and {Caino-Lores}, Silvina and Suarez, Kae and Ahn, Dong H. and Herbein, Stephen and Taufer, Michela},\n  year = {2022},\n  month = oct,\n  pages = {450--455},\n  doi = {10.1109/eScience55777.2022.00081},\n  abstract = {Workflows in High-Performance Computing (HPC) are rapidly changing towards more complex and large-scale workflows. In particular, high-throughput and ensemble workflows are becoming increasingly common. These workflows impose significant burden on current HPC scheduling systems which typically use slow, centralized schedulers. Generalized hierarchical scheduling (GHS) is a potential solution to face modern workflows but is not widely adopted in HPC yet. One difficulty hindering widespread adoption is the lack of performance models to configure and fit application requirements. The few existing models are often built on stick assumptions that can substantially reduce the analysis realism. In this paper, we reproduce the analysis and improve the realism of a state-of-the-art model presented in ``An Analytical Performance Model of Generalized Hierarchical Scheduling'' [1] by Herbein and co-authors. Specifically, we first reproduce four key analysis studies in the original paper and then expand the model by removing key assumptions, one at a time. In doing so, we extend the realism of the original model. We empirically validate our extended model using three different scenarios and discuss the observed accuracy.},\n  keywords = {Adaptation models,Analytical models,Computational modeling,Ensemble workflows,Hierarchical scheduling,High performance computing,Performance modeling,Predictive models,Processor scheduling,Scalability,Workflow analysis},\n  file = {/home/scainolo/Zotero/storage/MSTR3SHV/Luettgau et al. - 2022 - Reproducing and Extending Analytical Performance M.pdf;/home/scainolo/Zotero/storage/9VN6D932/9973677.html}\n}\n\n@inproceedings{wongRethinkingProgrammingParadigms2024,\n  title = {Rethinking {{Programming Paradigms}} in the {{QC-HPC Context}}},\n  author = {Wong, Elaine and {Caino-Lores}, Silvina and Chaves Claudino, Daniel and Dumitrescu, Eugene and Humble, Travis and Lopez Alarcon, Sonia},\n  year = {2024},\n  month = may,\n  publisher = {Oak Ridge National Laboratory (ORNL), Oak Ridge, TN (United States)},\n  url = {https://www.osti.gov/biblio/2376344},\n  urldate = {2024-07-26},\n  abstract = {Programming for today's quantum computers is making significant strides toward modern workflows compatible with high performance computing (HPC), but fundamental challenges still remain in the integration of these vastly different technologies. Quantum computing (QC) programming languages share some common ground, as well as their emerging runtimes and algorithmic modalities. In this short paper, we explore avenues of refinement for the quantum processing unit (QPU) in the context of many-tasks management, asynchronous or otherwise, in order to understand the value it can play in linking QC with HPC. Through examples, we illustrate how its potential for scientific discovery might be realized.},\n  langid = {english},\n  file = {/home/scainolo/Zotero/storage/7J4Y5ITP/Wong et al. - 2024 - Rethinking Programming Paradigms in the QC-HPC Con.pdf}\n}\n"}}]);